import json, numpy as np, re, requests, pickle, os
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from pyvi.ViTokenizer import tokenize
from chromadb import PersistentClient
import tiktoken
from hashlib import sha256

# ===== CONFIG =====
CHROMA_PATH = "/mnt/d/chatbot_vbpl/chroma_db"
CACHE_FILE = "/mnt/d/chatbot_vbpl/query_cache.pkl"

COLLECTION_NAME = "vbpl"
EMBED_MODEL_NAME = "AITeamVN/Vietnamese_Embedding"
OLLAMA_URL = "http://127.0.0.1:11434/api/generate"
LLM_MODEL = "llama3.2:3b"
TOP_K = 10
CONTEXT_TOKEN_LIMIT = 2500

# ===== LOAD MODEL =====
print("ğŸ”¹ Äang load model embedding...")
embed_model = SentenceTransformer(EMBED_MODEL_NAME, device="cuda")

# ===== CONNECT CHROMA =====
print("ğŸ”¹ Káº¿t ná»‘i ChromaDB...")
client = PersistentClient(path=CHROMA_PATH)
collection = client.get_collection(COLLECTION_NAME)

# ===== TOKEN LIMIT =====
tokenizer = tiktoken.get_encoding("cl100k_base")
def truncate_context_by_token(context, max_tokens=CONTEXT_TOKEN_LIMIT):
    tokens = tokenizer.encode(context)
    if len(tokens) <= max_tokens:
        return context
    return tokenizer.decode(tokens[:max_tokens]) + "\n...[rÃºt gá»n do vÆ°á»£t giá»›i háº¡n]"

# ===== QUERY NORMALIZATION =====
def normalize_query(q: str) -> str:
    q = q.strip().lower()
    q = re.sub(r"Ä‘iá»u\s*(\d+)", r"Äiá»u \1", q, flags=re.I)
    q = re.sub(r"chÆ°Æ¡ng\s*([IVXLCDM]+)", r"ChÆ°Æ¡ng \1", q, flags=re.I)
    q = re.sub(r"nghá»‹\s*Ä‘á»‹nh\s*(\d+)", r"Nghá»‹ Ä‘á»‹nh \1", q, flags=re.I)
    q = re.sub(r"luáº­t\s*(\d+)", r"Luáº­t \1", q, flags=re.I)
    return q

# ===== CACHE =====
if os.path.exists(CACHE_FILE):
    with open(CACHE_FILE, "rb") as f:
        query_cache = pickle.load(f)
else:
    query_cache = {}

def get_query_embedding(query: str) -> np.ndarray:
    """Tráº£ vá» vector embedding, cÃ³ cache"""
    key = sha256(query.encode("utf-8")).hexdigest()
    if key in query_cache:
        return np.array(query_cache[key])
    emb = embed_model.encode([tokenize(query)], convert_to_numpy=True)[0]
    query_cache[key] = emb
    with open(CACHE_FILE, "wb") as f:
        pickle.dump(query_cache, f)
    return emb

# ===== RERANK COSINE =====
def rerank_cosine(query_emb: np.ndarray, doc_embs: np.ndarray, docs, top_k: int = 5):
    sims = cosine_similarity(np.array([query_emb]), np.array(doc_embs))[0]
    ranked_idx = np.argsort(sims)[::-1][:top_k]
    return [{"text": docs[i]["text"], "meta": docs[i]["meta"], "score": float(sims[i])} for i in ranked_idx]

# ===== MAIN CHAT LOOP =====
print("\nğŸ¤– Chatbot VBPL sáºµn sÃ ng. GÃµ 'exit' Ä‘á»ƒ thoÃ¡t.\n")

while True:
    query = input("ğŸ‘¤ Báº¡n: ").strip()
    if query.lower() in ["exit", "quit", "q"]:
        print("Táº¡m biá»‡t nhÃ© ğŸ‘‹")
        break

    norm_query = normalize_query(query)
    q_emb = get_query_embedding(norm_query)

    # --- Truy váº¥n Chroma ---
    match = re.search(r"Ä‘iá»u\s*\d+", norm_query)
    where_filter = {"$contains": match.group(0)} if match else None

    if where_filter:
        results = collection.query(
            query_embeddings=[q_emb.tolist()],
            n_results=TOP_K * 2,
            where_document=where_filter,  # pyright: ignore[reportArgumentType]
            include=["documents", "metadatas"]
        )
    else:
        results = collection.query(
            query_embeddings=[q_emb.tolist()],
            n_results=TOP_K * 2,
            include=["documents", "metadatas"]
        )

    docs_raw = (results.get("documents") or [[]])[0]
    metas_raw = (results.get("metadatas") or [[]])[0]

    if not docs_raw:
        print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u phÃ¹ há»£p.\n")
        continue

    # --- Rerank cosine ---
    doc_embs = embed_model.encode([tokenize(d) for d in docs_raw], convert_to_numpy=True)
    docs_struct = [{"text": d, "meta": m} for d, m in zip(docs_raw, metas_raw)]
    top_docs = rerank_cosine(q_emb, doc_embs, docs_struct, top_k=5)

    print("\n Top 3 Ä‘oáº¡n Ä‘Æ°á»£c chá»n (sau rerank):")
    for i, d in enumerate(top_docs[:3], 1):
        title = d["meta"].get("title", "KhÃ´ng rÃµ")
        sec = d["meta"].get("section_path", "")
        print(f"{i}. {title} ({sec}) | cosine={d['score']:.4f}")
    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")

    # --- Context ---
    context = "\n\n".join([
        f"ğŸ“˜ {d['meta'].get('title','')} ({d['meta'].get('section_path','')})\n{d['text']}"
        for d in top_docs[:3]
    ])
    context = truncate_context_by_token(context)

    # --- Prompt ---
    prompt = f"""
ğŸ“˜ NGá»® Cáº¢NH (trÃ­ch tá»« cÃ¡c vÄƒn báº£n phÃ¡p luáº­t Viá»‡t Nam):
{context}

ğŸ§© CÃ‚U Há»I:
{query}

---
HÃ£y tráº£ lá»i hoÃ n toÃ n báº±ng **tiáº¿ng Viá»‡t chuáº©n phÃ¡p lÃ½**, rÃµ rÃ ng vÃ  chÃ­nh xÃ¡c.
Dá»±a vÃ o NGá»® Cáº¢NH á»Ÿ trÃªn Ä‘á»ƒ **trÃ­ch dáº«n hoáº·c tÃ³m táº¯t ná»™i dung liÃªn quan nháº¥t** Ä‘áº¿n cÃ¢u há»i.

YÃªu cáº§u:
- Tuyá»‡t Ä‘á»‘i **khÃ´ng sá»­ dá»¥ng tiáº¿ng nÆ°á»›c ngoÃ i** (Ä‘áº·c biá»‡t lÃ  tiáº¿ng Trung hoáº·c tiáº¿ng Anh).
- Giá»¯ giá»ng vÄƒn nghiÃªm tÃºc, trung láº­p, vÃ  thá»ƒ hiá»‡n Ä‘Ãºng phong cÃ¡ch hÃ nh chÃ­nh - phÃ¡p lÃ½.
- **TrÃ­ch dáº«n rÃµ rÃ ng** tÃªn vÄƒn báº£n, Ä‘iá»u luáº­t hoáº·c chÆ°Æ¡ng/má»¥c náº¿u cÃ³ trong NGá»® Cáº¢NH.
- Náº¿u NGá»® Cáº¢NH chá»‰ cung cáº¥p má»™t pháº§n thÃ´ng tin, hÃ£y diá»…n giáº£i há»£p lÃ½ dá»±a trÃªn ná»™i dung Ä‘Ã³, khÃ´ng thÃªm Ã½ kiáº¿n cÃ¡ nhÃ¢n.
- Náº¿u hoÃ n toÃ n **khÃ´ng cÃ³ thÃ´ng tin liÃªn quan**, chá»‰ khi Ä‘Ã³ má»›i tráº£ lá»i:
  "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin trong cÃ¡c vÄƒn báº£n Ä‘Æ°á»£c cung cáº¥p."
---
"""

    try:
        payload = {"model": LLM_MODEL, "prompt": prompt, "stream": False}
        res = requests.post(OLLAMA_URL, json=payload)
        if res.status_code == 200:
            answer = res.json().get("response", "").strip()
            print(f"\nğŸ§  {answer}\n")
        else:
            print(f"âŒ Lá»—i Ollama ({res.status_code}): {res.text}\n")
    except Exception as e:
        print(f"âŒ Exception: {e}\n")
