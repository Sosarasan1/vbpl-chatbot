import json, numpy as np, re, requests, pickle, os
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from pyvi.ViTokenizer import tokenize
from chromadb import PersistentClient
import tiktoken
from hashlib import sha256

# ===== CONFIG =====
CHROMA_PATH = "/mnt/d/chatbot_vbpl/chroma_db"
CACHE_FILE = "/mnt/d/chatbot_vbpl/query_cache.pkl"

COLLECTION_NAME = "vbpl"
EMBED_MODEL_NAME = "AITeamVN/Vietnamese_Embedding"
OLLAMA_URL = "http://127.0.0.1:11434/api/generate"
LLM_MODEL = "llama3.2:3b"
TOP_K = 10
CONTEXT_TOKEN_LIMIT = 2500
SIM_THRESHOLD = 0.35   # ğŸ”¹ NgÆ°á»¡ng tÆ°Æ¡ng Ä‘á»“ng tá»‘i thiá»ƒu

# ===== LOAD MODEL =====
print("ğŸ”¹ Äang load model embedding...")
embed_model = SentenceTransformer(EMBED_MODEL_NAME, device="cuda")

# ===== CONNECT CHROMA =====
print("ğŸ”¹ Káº¿t ná»‘i ChromaDB...")
client = PersistentClient(path=CHROMA_PATH)
collection = client.get_collection(COLLECTION_NAME)

# ===== TOKEN LIMIT =====
tokenizer = tiktoken.get_encoding("cl100k_base")
def truncate_context_by_token(context, max_tokens=CONTEXT_TOKEN_LIMIT):
    tokens = tokenizer.encode(context)
    if len(tokens) <= max_tokens:
        return context
    return tokenizer.decode(tokens[:max_tokens]) + "\n...[rÃºt gá»n do vÆ°á»£t giá»›i háº¡n]"

# ===== QUERY NORMALIZATION =====
def normalize_query(q: str) -> str:
    q = q.strip().lower()
    q = re.sub(r"Ä‘iá»u\s*(\d+)", r"Äiá»u \1", q, flags=re.I)
    q = re.sub(r"chÆ°Æ¡ng\s*([IVXLCDM]+)", r"ChÆ°Æ¡ng \1", q, flags=re.I)
    q = re.sub(r"nghá»‹\s*Ä‘á»‹nh\s*(\d+)", r"Nghá»‹ Ä‘á»‹nh \1", q, flags=re.I)
    q = re.sub(r"luáº­t\s*(\d+)", r"Luáº­t \1", q, flags=re.I)
    return q

# ===== CACHE =====
if os.path.exists(CACHE_FILE):
    with open(CACHE_FILE, "rb") as f:
        query_cache = pickle.load(f)
else:
    query_cache = {}

def get_query_embedding(query: str) -> np.ndarray:
    """Tráº£ vá» vector embedding, cÃ³ cache"""
    key = sha256(query.encode("utf-8")).hexdigest()
    if key in query_cache:
        return np.array(query_cache[key])
    emb = embed_model.encode([tokenize(query)], convert_to_numpy=True)[0]
    query_cache[key] = emb
    with open(CACHE_FILE, "wb") as f:
        pickle.dump(query_cache, f)
    return emb

# ===== RERANK COSINE =====
def rerank_cosine(query_emb: np.ndarray, doc_embs: np.ndarray, docs, top_k: int = 5):
    sims = cosine_similarity(np.array([query_emb]), np.array(doc_embs))[0]
    ranked_idx = np.argsort(sims)[::-1][:top_k]
    return [{"text": docs[i]["text"], "meta": docs[i]["meta"], "score": float(sims[i])} for i in ranked_idx]

# ===== MAIN CHAT LOOP =====
print("\nğŸ¤– Chatbot VBPL sáºµn sÃ ng. GÃµ 'exit' Ä‘á»ƒ thoÃ¡t.\n")

while True:
    query = input("ğŸ‘¤ Báº¡n: ").strip()
    if query.lower() in ["exit", "quit", "q"]:
        print("Táº¡m biá»‡t nhÃ© ğŸ‘‹")
        break

    norm_query = normalize_query(query)
    q_emb = get_query_embedding(norm_query)

    # --- Truy váº¥n Chroma ---
    match = re.search(r"Ä‘iá»u\s*\d+", norm_query)
    where_filter = {"$contains": match.group(0)} if match else None

    if where_filter:
        results = collection.query(
            query_embeddings=[q_emb.tolist()],
            n_results=TOP_K * 2,
            where_document=where_filter,
            include=["documents", "metadatas"]
        )
    else:
        results = collection.query(
            query_embeddings=[q_emb.tolist()],
            n_results=TOP_K * 2,
            include=["documents", "metadatas"]
        )

    docs_raw = (results.get("documents") or [[]])[0]
    metas_raw = (results.get("metadatas") or [[]])[0]

    if not docs_raw:
        print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u phÃ¹ há»£p.\n")
        continue

    # --- Rerank cosine ---
    doc_embs = embed_model.encode([tokenize(d) for d in docs_raw], convert_to_numpy=True)
    docs_struct = [{"text": d, "meta": m} for d, m in zip(docs_raw, metas_raw)]
    top_docs = rerank_cosine(q_emb, doc_embs, docs_struct, top_k=5)

    # ğŸ”¹ Kiá»ƒm tra ngÆ°á»¡ng tÆ°Æ¡ng Ä‘á»“ng
    avg_sim = np.mean([d["score"] for d in top_docs])
    if avg_sim < SIM_THRESHOLD:
        print("ğŸ¤” CÃ¢u há»i nÃ y cÃ³ váº» khÃ´ng thuá»™c pháº¡m vi phÃ¡p luáº­t Viá»‡t Nam.\n"
              "TÃ´i chá»‰ cung cáº¥p thÃ´ng tin dá»±a trÃªn cÃ¡c vÄƒn báº£n phÃ¡p luáº­t Ä‘Æ°á»£c lÆ°u trá»¯ trong cÆ¡ sá»Ÿ dá»¯ liá»‡u.\n")
        continue

    # Lá»c chá»‰ giá»¯ Ä‘oáº¡n liÃªn quan
    top_docs = [d for d in top_docs if d["score"] >= SIM_THRESHOLD]
    if not top_docs:
        print("âš ï¸ KhÃ´ng cÃ³ Ä‘oáº¡n nÃ o Ä‘á»§ liÃªn quan Ä‘á»ƒ tráº£ lá»i.\n")
        continue

    print("\n Top 3 Ä‘oáº¡n Ä‘Æ°á»£c chá»n (sau rerank):")
    for i, d in enumerate(top_docs[:3], 1):
        title = d["meta"].get("title", "KhÃ´ng rÃµ")
        sec = d["meta"].get("section_path", "")
        print(f"{i}. {title} ({sec}) | cosine={d['score']:.4f}")
    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")

    # --- Context ---
    context = "\n\n".join([
        f"ğŸ“˜ {d['meta'].get('title','')} ({d['meta'].get('section_path','')})\n{d['text']}"
        for d in top_docs[:3]
    ])
    context = truncate_context_by_token(context)

    # --- Prompt ---
    prompt = f"""
ğŸ“˜ NGá»® Cáº¢NH (trÃ­ch tá»« cÃ¡c vÄƒn báº£n phÃ¡p luáº­t Viá»‡t Nam):
{context}

ğŸ§© CÃ‚U Há»I:
{query}

---
HÃ£y tráº£ lá»i hoÃ n toÃ n báº±ng **tiáº¿ng Viá»‡t chuáº©n phÃ¡p lÃ½**, rÃµ rÃ ng vÃ  chÃ­nh xÃ¡c.
Dá»±a vÃ o NGá»® Cáº¢NH á»Ÿ trÃªn Ä‘á»ƒ **trÃ­ch dáº«n hoáº·c tÃ³m táº¯t ná»™i dung liÃªn quan nháº¥t** Ä‘áº¿n cÃ¢u há»i.

YÃªu cáº§u:
- Náº¿u khÃ´ng cÃ³ thÃ´ng tin liÃªn quan, hÃ£y tráº£ lá»i:
  "CÃ¢u há»i nÃ y khÃ´ng náº±m trong pháº¡m vi vÄƒn báº£n phÃ¡p luáº­t Viá»‡t Nam."
- KhÃ´ng bá»‹a Ä‘áº·t hoáº·c suy luáº­n ngoÃ i ná»™i dung NGá»® Cáº¢NH.
- Giá»¯ giá»ng vÄƒn hÃ nh chÃ­nh, phÃ¡p lÃ½, trung láº­p.
---
"""

    try:
        payload = {"model": LLM_MODEL, "prompt": prompt, "stream": False}
        res = requests.post(OLLAMA_URL, json=payload)
        if res.status_code == 200:
            answer = res.json().get("response", "").strip()
            print(f"\nğŸ§  {answer}\n")
        else:
            print(f"âŒ Lá»—i Ollama ({res.status_code}): {res.text}\n")
    except Exception as e:
        print(f"âŒ Exception: {e}\n")
